"""Google Cloud Vision AI document scanner and explainer for HaqSetu.

UNIQUE FEATURE: No other platform in India lets illiterate or semi-literate
citizens photograph a government document -- an FIR, court notice, land
record, eviction order, pension rejection letter -- and receive an instant,
plain-language audio explanation of what it says, what it means for them,
what deadlines they face, and exactly what they need to do next, in their
own language.

Architecture:
    * Uses Google Cloud Vision API for robust multi-script OCR across all
      22 scheduled Indian languages (Devanagari, Bengali, Tamil, Telugu,
      Kannada, Malayalam, Gurmukhi, Odia, Gujarati, Urdu/Nastaliq, etc.).
    * Pipes extracted text through Gemini to classify the document type,
      identify referenced laws/schemes, extract deadlines and action items,
      and generate a jargon-free summary.
    * Translates the explanation into the user's preferred language via
      the ``TranslationService``.
    * Produces a "plain language summary" optimised for text-to-speech
      delivery -- short sentences, spelled-out abbreviations, pause-
      friendly structure -- so that even a user who cannot read can
      understand the document through audio playback.

Privacy:
    * Image data is processed ephemerally -- never written to GCS, disk,
      or logs.  Only the extracted text (required for the LLM call) is
      held in memory for the duration of the request.
"""

from __future__ import annotations

import json
import time
from enum import StrEnum
from typing import TYPE_CHECKING, Final

import structlog
from google.cloud import vision
from pydantic import BaseModel, Field
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

if TYPE_CHECKING:
    from src.services.llm import LLMService
    from src.services.translation import TranslationService

logger: structlog.stdlib.BoundLogger = structlog.get_logger(__name__)

# ---------------------------------------------------------------------------
# Legal disclaimer -- must accompany every document explanation
# ---------------------------------------------------------------------------

LEGAL_DISCLAIMER: Final[str] = (
    "DISCLAIMER: This document scan and explanation is generated by an AI system "
    "for informational and educational purposes only. It is NOT a certified "
    "translation, NOT a legal interpretation, and must NOT be presented as evidence "
    "in any legal proceeding. The accuracy of the OCR and the AI-generated "
    "explanation cannot be guaranteed, especially for handwritten, damaged, or "
    "low-quality documents. For authoritative interpretation of any government "
    "document, court notice, or legal order, please consult a qualified advocate "
    "or contact your nearest District Legal Services Authority (DLSA) at helpline "
    "1516. Free legal aid is available under the Legal Services Authorities Act, "
    "1987 for eligible citizens."
)

LEGAL_DISCLAIMER_HI: Final[str] = (
    "अस्वीकरण: यह दस्तावेज़ स्कैन और व्याख्या एक AI प्रणाली द्वारा केवल "
    "सूचनात्मक और शैक्षिक उद्देश्यों के लिए तैयार की गई है। यह प्रमाणित "
    "अनुवाद नहीं है, कानूनी व्याख्या नहीं है, और किसी भी कानूनी कार्यवाही में "
    "साक्ष्य के रूप में प्रस्तुत नहीं की जानी चाहिए। किसी भी सरकारी दस्तावेज़, "
    "अदालती नोटिस, या कानूनी आदेश की आधिकारिक व्याख्या के लिए कृपया एक योग्य "
    "अधिवक्ता से परामर्श करें या अपने निकटतम जिला विधिक सेवा प्राधिकरण (DLSA) "
    "से हेल्पलाइन 1516 पर संपर्क करें।"
)

# ---------------------------------------------------------------------------
# Document type classification
# ---------------------------------------------------------------------------


class DocumentType(StrEnum):
    """Known government document categories that HaqSetu can explain."""

    __slots__ = ()

    FIR = "fir"
    COURT_NOTICE = "court_notice"
    COURT_ORDER = "court_order"
    LAND_RECORD = "land_record"
    REVENUE_RECORD = "revenue_record"
    EVICTION_NOTICE = "eviction_notice"
    PENSION_ORDER = "pension_order"
    RATION_CARD = "ration_card"
    CASTE_CERTIFICATE = "caste_certificate"
    INCOME_CERTIFICATE = "income_certificate"
    DOMICILE_CERTIFICATE = "domicile_certificate"
    BIRTH_CERTIFICATE = "birth_certificate"
    DEATH_CERTIFICATE = "death_certificate"
    GOVERNMENT_LETTER = "government_letter"
    GOVERNMENT_ORDER = "government_order"
    SHOW_CAUSE_NOTICE = "show_cause_notice"
    DEMAND_NOTICE = "demand_notice"
    TAX_NOTICE = "tax_notice"
    CHALLAN = "challan"
    PROPERTY_DEED = "property_deed"
    MUTATION_ORDER = "mutation_order"
    BANK_NOTICE = "bank_notice"
    INSURANCE_DOCUMENT = "insurance_document"
    SCHEME_SANCTION_LETTER = "scheme_sanction_letter"
    SCHEME_REJECTION_LETTER = "scheme_rejection_letter"
    EMPLOYMENT_LETTER = "employment_letter"
    LABOUR_NOTICE = "labour_notice"
    POLICE_COMPLAINT = "police_complaint"
    BAIL_ORDER = "bail_order"
    SUMMONS = "summons"
    WARRANT = "warrant"
    UNKNOWN = "unknown"


# ---------------------------------------------------------------------------
# Priority levels for action items
# ---------------------------------------------------------------------------


class ActionPriority(StrEnum):
    """Urgency level of an action item extracted from a document."""

    __slots__ = ()

    CRITICAL = "critical"   # Warrants, arrest orders, emergency hearings
    HIGH = "high"           # Court dates, FIR deadlines, eviction dates
    MEDIUM = "medium"       # Response deadlines, document submission
    LOW = "low"             # Informational, no immediate deadline


# ---------------------------------------------------------------------------
# Pydantic models
# ---------------------------------------------------------------------------


class ActionItem(BaseModel):
    """A concrete action the document requires the user to take."""

    description: str
    deadline: str | None = None
    priority: ActionPriority = ActionPriority.MEDIUM
    contact_info: str | None = None


class ScanResult(BaseModel):
    """Result of OCR scanning a document image."""

    extracted_text: str
    detected_language: str
    confidence: float = Field(ge=0.0, le=1.0)
    document_type: DocumentType = DocumentType.UNKNOWN
    page_count: int = 1
    processing_time_ms: float = 0.0


class DocumentExplanation(BaseModel):
    """Full explanation of a scanned document, ready for TTS delivery."""

    summary: str
    action_items: list[ActionItem] = Field(default_factory=list)
    deadlines: list[str] = Field(default_factory=list)
    referenced_laws: list[str] = Field(default_factory=list)
    referenced_schemes: list[str] = Field(default_factory=list)
    plain_language_summary: str = ""
    original_text: str = ""
    document_type: DocumentType = DocumentType.UNKNOWN
    detected_language: str = ""
    explanation_language: str = ""
    confidence: float = 0.0
    disclaimer: str = LEGAL_DISCLAIMER
    processing_time_ms: float = 0.0


# ---------------------------------------------------------------------------
# Prompts
# ---------------------------------------------------------------------------

_DOCUMENT_ANALYSIS_PROMPT: Final[str] = """\
You are HaqSetu Document Analyzer, an expert at reading Indian government \
documents and explaining them in simple language that any citizen can \
understand, including those with limited formal education.

You have been given the text extracted via OCR from a photograph of a \
government document. The OCR may contain errors, missing characters, or \
garbled text -- use your judgment to reconstruct the intended meaning.

TASK: Analyze the document and return a JSON object with these keys:

{{
  "document_type": "<one of: fir, court_notice, court_order, land_record, \
revenue_record, eviction_notice, pension_order, ration_card, \
caste_certificate, income_certificate, domicile_certificate, \
birth_certificate, death_certificate, government_letter, government_order, \
show_cause_notice, demand_notice, tax_notice, challan, property_deed, \
mutation_order, bank_notice, insurance_document, scheme_sanction_letter, \
scheme_rejection_letter, employment_letter, labour_notice, \
police_complaint, bail_order, summons, warrant, unknown>",
  "summary": "<A 3-5 sentence summary of what this document is, who issued \
it, who it is addressed to, and what it means for the recipient. Use \
simple, everyday language. Do NOT use legal jargon.>",
  "action_items": [
    {{
      "description": "<What the person needs to do>",
      "deadline": "<Date or timeframe if mentioned, or null>",
      "priority": "<critical/high/medium/low>",
      "contact_info": "<Phone number, address, or office to contact, or null>"
    }}
  ],
  "deadlines": ["<List of all dates and deadlines mentioned in the document>"],
  "referenced_laws": ["<List of all laws, acts, sections, BNS/IPC sections, \
rules, and regulations referenced in the document>"],
  "referenced_schemes": ["<List of all government schemes, yojanas, or \
welfare programs referenced in the document>"],
  "plain_language_summary": "<A very simple explanation suitable for someone \
who cannot read. This will be read aloud. Keep sentences under 15 words. \
Spell out all abbreviations. Use pauses between key points. Start with \
'This is a...' and explain what the person should do. End with who they \
can call or visit for help.>"
}}

IMPORTANT RULES:
1. If the document mentions ANY deadline or court date, flag it as a \
high-priority action item.
2. If the document is a warrant, summons, or arrest-related, flag all \
action items as critical priority.
3. Always include at least one action item, even if it is just \
"Keep this document safe for your records".
4. For the plain language summary, imagine you are explaining this to \
your grandmother who cannot read. Be warm, clear, and reassuring.
5. If the document references laws by old IPC/CrPC numbers, also \
mention the equivalent BNS/BNSS sections if you know them.
6. If the OCR text is very garbled or unreadable, say so honestly \
and recommend the user visit a CSC or tehsil office for help.

EXTRACTED TEXT FROM DOCUMENT:
{extracted_text}
"""

_TRANSLATE_EXPLANATION_PROMPT: Final[str] = """\
Translate the following document explanation into {target_language}. \
Keep it simple and natural. This will be read aloud to someone who \
may not be able to read. Use everyday words, not formal or literary \
language. Keep sentence structure simple.

Text to translate:
{text}
"""


# ---------------------------------------------------------------------------
# DocumentScannerService
# ---------------------------------------------------------------------------


class DocumentScannerService:
    """Google Cloud Vision + Gemini document scanning and explanation service.

    Scans photographs of government documents (FIRs, court notices, land
    records, pension letters, etc.), extracts text via OCR, classifies the
    document, identifies action items and deadlines, and generates a plain-
    language explanation in the user's preferred language.

    The ``plain_language_summary`` field in the result is specifically
    optimised for text-to-speech delivery to illiterate users.

    IMPORTANT: Every response includes a legal disclaimer. This service
    does NOT provide legal interpretation or certified translation.

    Parameters
    ----------
    project_id:
        GCP project identifier.
    llm:
        An :class:`LLMService` instance for Gemini-powered document
        analysis and explanation.
    translation:
        An optional :class:`TranslationService` instance for translating
        explanations into the user's preferred language.
    """

    __slots__ = ("_client", "_llm", "_project_id", "_translation")

    def __init__(
        self,
        project_id: str,
        llm: LLMService,
        translation: TranslationService | None = None,
    ) -> None:
        self._project_id = project_id
        self._llm = llm
        self._translation = translation
        self._client: vision.ImageAnnotatorAsyncClient | None = None

    # -- client lifecycle ---------------------------------------------------

    async def _get_client(self) -> vision.ImageAnnotatorAsyncClient:
        """Lazily initialize the Vision API async client."""
        if self._client is None:
            self._client = vision.ImageAnnotatorAsyncClient()
            logger.info(
                "document_scanner.client_initialized",
                project=self._project_id,
            )
        return self._client

    # -- public API ---------------------------------------------------------

    @retry(
        retry=retry_if_exception_type(Exception),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=8),
        reraise=True,
    )
    async def scan_document(
        self,
        image_data: bytes,
        language: str = "auto",
    ) -> ScanResult:
        """Extract text from a document image using Google Cloud Vision OCR.

        Parameters
        ----------
        image_data:
            Raw image bytes (JPEG, PNG, TIFF, BMP, or PDF first page).
        language:
            Hint for the OCR engine.  ``"auto"`` lets Vision detect the
            script automatically.  Pass an ISO 639-1 code (e.g. ``"hi"``,
            ``"ta"``) to bias detection toward a specific language.

        Returns
        -------
        ScanResult
            Extracted text, detected language, OCR confidence, and
            preliminary document type classification.
        """
        start = time.perf_counter()
        client = await self._get_client()

        image = vision.Image(content=image_data)

        # Build language hints for the OCR engine.
        image_context = None
        if language != "auto":
            language_hints = _build_language_hints(language)
            image_context = vision.ImageContext(
                language_hints=language_hints,
            )

        # Use DOCUMENT_TEXT_DETECTION for best results on printed documents.
        # This feature uses the dense text detection model optimised for
        # documents rather than scene text (e.g. street signs).
        request = vision.AnnotateImageRequest(
            image=image,
            features=[
                vision.Feature(
                    type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION,
                ),
            ],
            image_context=image_context,
        )

        batch_request = vision.BatchAnnotateImagesRequest(requests=[request])

        logger.debug(
            "document_scanner.ocr_request",
            image_bytes=len(image_data),
            language_hint=language,
        )

        response = await client.batch_annotate_images(request=batch_request)

        elapsed_ms = (time.perf_counter() - start) * 1000

        # Extract results from the first (only) image response.
        image_response = response.responses[0]

        if image_response.error.message:
            logger.error(
                "document_scanner.ocr_error",
                error=image_response.error.message,
            )
            return ScanResult(
                extracted_text="",
                detected_language="und",
                confidence=0.0,
                document_type=DocumentType.UNKNOWN,
                processing_time_ms=round(elapsed_ms, 2),
            )

        full_text_annotation = image_response.full_text_annotation
        if not full_text_annotation or not full_text_annotation.text:
            logger.warning(
                "document_scanner.ocr_no_text",
                image_bytes=len(image_data),
            )
            return ScanResult(
                extracted_text="",
                detected_language="und",
                confidence=0.0,
                document_type=DocumentType.UNKNOWN,
                processing_time_ms=round(elapsed_ms, 2),
            )

        extracted_text = full_text_annotation.text

        # Determine detected language and confidence from page-level info.
        detected_language = "und"
        confidence = 0.0
        page_count = len(full_text_annotation.pages)

        if full_text_annotation.pages:
            first_page = full_text_annotation.pages[0]
            if first_page.property and first_page.property.detected_languages:
                top_lang = first_page.property.detected_languages[0]
                detected_language = top_lang.language_code
                confidence = top_lang.confidence

            # Compute average block-level confidence as a proxy for
            # overall OCR quality.
            block_confidences: list[float] = []
            for page in full_text_annotation.pages:
                for block in page.blocks:
                    if block.confidence > 0:
                        block_confidences.append(block.confidence)
            if block_confidences:
                confidence = sum(block_confidences) / len(block_confidences)

        # Preliminary document type classification based on keywords
        # in the extracted text.  The LLM will refine this later.
        document_type = _classify_document_type(extracted_text)

        result = ScanResult(
            extracted_text=extracted_text,
            detected_language=detected_language,
            confidence=round(min(confidence, 1.0), 3),
            document_type=document_type,
            page_count=page_count,
            processing_time_ms=round(elapsed_ms, 2),
        )

        logger.info(
            "document_scanner.ocr_complete",
            text_length=len(extracted_text),
            detected_language=detected_language,
            confidence=result.confidence,
            document_type=document_type.value,
            page_count=page_count,
            processing_time_ms=result.processing_time_ms,
        )

        return result

    @retry(
        retry=retry_if_exception_type(Exception),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=8),
        reraise=True,
    )
    async def explain_document(
        self,
        scan_result: ScanResult,
        language: str = "hi",
    ) -> DocumentExplanation:
        """Generate a plain-language explanation of a scanned document.

        Takes the output of :meth:`scan_document` and uses Gemini to
        produce a structured explanation with action items, deadlines,
        referenced laws, and a TTS-optimised plain-language summary.

        Parameters
        ----------
        scan_result:
            The :class:`ScanResult` from a prior ``scan_document`` call.
        language:
            ISO 639-1 code of the language in which the explanation should
            be delivered (e.g. ``"hi"`` for Hindi, ``"ta"`` for Tamil).

        Returns
        -------
        DocumentExplanation
            Full explanation with summary, action items, deadlines,
            referenced laws/schemes, and a TTS-friendly plain-language
            summary.
        """
        start = time.perf_counter()

        if not scan_result.extracted_text.strip():
            logger.warning("document_scanner.explain_empty_text")
            elapsed_ms = (time.perf_counter() - start) * 1000
            return DocumentExplanation(
                summary="The document image could not be read. The text "
                        "was too blurry, damaged, or unclear for the scanner "
                        "to extract. Please try taking a clearer photograph "
                        "with good lighting, or visit your nearest Common "
                        "Service Centre (CSC) for help.",
                action_items=[
                    ActionItem(
                        description="Take a clearer photograph of the document "
                                    "with good lighting and try again, or visit "
                                    "your nearest CSC or tehsil office for help.",
                        priority=ActionPriority.MEDIUM,
                    ),
                ],
                plain_language_summary="The document photo was not clear enough "
                                      "to read. Please try again with a clearer "
                                      "photo. You can also visit your nearest "
                                      "Common Service Centre for help.",
                original_text="",
                document_type=DocumentType.UNKNOWN,
                detected_language=scan_result.detected_language,
                explanation_language=language,
                confidence=0.0,
                disclaimer=_get_disclaimer(language),
                processing_time_ms=round(elapsed_ms, 2),
            )

        # Build the analysis prompt with the extracted text.
        prompt = _DOCUMENT_ANALYSIS_PROMPT.format(
            extracted_text=scan_result.extracted_text,
        )

        # Call Gemini for structured analysis.
        try:
            llm_result = await self._llm.generate(
                prompt=prompt,
                temperature=0.2,
            )
            raw_response = llm_result.answer
        except Exception:
            logger.error("document_scanner.llm_failed", exc_info=True)
            elapsed_ms = (time.perf_counter() - start) * 1000
            return DocumentExplanation(
                summary="We were unable to analyze this document right now. "
                        "Please try again in a few minutes.",
                action_items=[
                    ActionItem(
                        description="Try scanning the document again. If the "
                                    "problem persists, visit your nearest CSC "
                                    "or contact the Tele-Law helpline at 1516.",
                        priority=ActionPriority.MEDIUM,
                        contact_info="Tele-Law helpline: 1516",
                    ),
                ],
                plain_language_summary="We could not read this document right "
                                      "now. Please try again later. For help, "
                                      "call Tele-Law helpline at 1516.",
                original_text=scan_result.extracted_text,
                document_type=scan_result.document_type,
                detected_language=scan_result.detected_language,
                explanation_language=language,
                confidence=scan_result.confidence,
                disclaimer=_get_disclaimer(language),
                processing_time_ms=round(elapsed_ms, 2),
            )

        # Parse the structured JSON response from Gemini.
        analysis = _parse_analysis_response(raw_response)

        # Extract document type from LLM response (it may be more accurate
        # than the keyword-based classifier).
        llm_doc_type = analysis.get("document_type", "")
        try:
            document_type = DocumentType(llm_doc_type)
        except ValueError:
            document_type = scan_result.document_type

        # Build action items.
        action_items: list[ActionItem] = []
        for item_dict in analysis.get("action_items", []):
            if isinstance(item_dict, dict):
                priority_str = item_dict.get("priority", "medium")
                try:
                    priority = ActionPriority(priority_str)
                except ValueError:
                    priority = ActionPriority.MEDIUM

                action_items.append(
                    ActionItem(
                        description=item_dict.get("description", ""),
                        deadline=item_dict.get("deadline"),
                        priority=priority,
                        contact_info=item_dict.get("contact_info"),
                    )
                )

        # If no action items were extracted, add a default one.
        if not action_items:
            action_items.append(
                ActionItem(
                    description="Keep this document safe for your records. "
                                "If you have questions, visit your nearest "
                                "Common Service Centre (CSC) or call "
                                "Tele-Law helpline at 1516.",
                    priority=ActionPriority.LOW,
                    contact_info="Tele-Law helpline: 1516",
                )
            )

        summary = analysis.get("summary", "")
        deadlines = analysis.get("deadlines", [])
        referenced_laws = analysis.get("referenced_laws", [])
        referenced_schemes = analysis.get("referenced_schemes", [])
        plain_language_summary = analysis.get("plain_language_summary", "")

        # Ensure lists contain only strings.
        deadlines = [str(d) for d in deadlines if d]
        referenced_laws = [str(law) for law in referenced_laws if law]
        referenced_schemes = [str(s) for s in referenced_schemes if s]

        # Translate the explanation into the user's preferred language if
        # it differs from English (the LLM generates English by default).
        if language != "en" and self._translation is not None:
            summary = await self._translate_text(summary, language)
            plain_language_summary = await self._translate_text(
                plain_language_summary, language
            )
            action_items = await self._translate_action_items(
                action_items, language
            )
            deadlines = [
                await self._translate_text(d, language) for d in deadlines
            ]

        elapsed_ms = (time.perf_counter() - start) * 1000

        explanation = DocumentExplanation(
            summary=summary,
            action_items=action_items,
            deadlines=deadlines,
            referenced_laws=referenced_laws,
            referenced_schemes=referenced_schemes,
            plain_language_summary=plain_language_summary,
            original_text=scan_result.extracted_text,
            document_type=document_type,
            detected_language=scan_result.detected_language,
            explanation_language=language,
            confidence=scan_result.confidence,
            disclaimer=_get_disclaimer(language),
            processing_time_ms=round(elapsed_ms, 2),
        )

        logger.info(
            "document_scanner.explain_complete",
            document_type=document_type.value,
            action_items=len(action_items),
            deadlines=len(deadlines),
            laws=len(referenced_laws),
            schemes=len(referenced_schemes),
            language=language,
            processing_time_ms=explanation.processing_time_ms,
        )

        return explanation

    async def scan_and_explain(
        self,
        image_data: bytes,
        language: str = "hi",
    ) -> DocumentExplanation:
        """Scan a document image and explain it in one call.

        Convenience method that chains :meth:`scan_document` and
        :meth:`explain_document`.

        Parameters
        ----------
        image_data:
            Raw image bytes.
        language:
            ISO 639-1 code of the preferred explanation language.

        Returns
        -------
        DocumentExplanation
            Full explanation including action items, deadlines, and a
            TTS-friendly plain-language summary.
        """
        start = time.perf_counter()

        logger.info(
            "document_scanner.scan_and_explain_start",
            image_bytes=len(image_data),
            language=language,
        )

        scan_result = await self.scan_document(
            image_data=image_data,
            language=language,
        )

        explanation = await self.explain_document(
            scan_result=scan_result,
            language=language,
        )

        # Override processing time to reflect the total pipeline duration.
        total_ms = (time.perf_counter() - start) * 1000
        explanation.processing_time_ms = round(total_ms, 2)

        logger.info(
            "document_scanner.scan_and_explain_complete",
            document_type=explanation.document_type.value,
            detected_language=explanation.detected_language,
            explanation_language=explanation.explanation_language,
            confidence=explanation.confidence,
            action_items=len(explanation.action_items),
            total_processing_time_ms=explanation.processing_time_ms,
        )

        return explanation

    async def close(self) -> None:
        """Release underlying gRPC resources."""
        if self._client is not None:
            transport = self._client.transport
            if hasattr(transport, "close"):
                await transport.close()  # type: ignore[misc]
            self._client = None

    # -- internal helpers ---------------------------------------------------

    async def _translate_text(self, text: str, target_language: str) -> str:
        """Translate text to the target language, falling back to original."""
        if not text or self._translation is None:
            return text
        try:
            return await self._translation.translate(
                text=text,
                source_lang="en",
                target_lang=target_language,
            )
        except Exception:
            logger.warning(
                "document_scanner.translation_failed",
                target_language=target_language,
                text_length=len(text),
                exc_info=True,
            )
            return text

    async def _translate_action_items(
        self,
        action_items: list[ActionItem],
        target_language: str,
    ) -> list[ActionItem]:
        """Translate action item descriptions to the target language."""
        if self._translation is None:
            return action_items

        translated: list[ActionItem] = []
        for item in action_items:
            translated_desc = await self._translate_text(
                item.description, target_language
            )
            translated_contact = item.contact_info
            if item.contact_info:
                translated_contact = await self._translate_text(
                    item.contact_info, target_language
                )
            translated.append(
                ActionItem(
                    description=translated_desc,
                    deadline=item.deadline,
                    priority=item.priority,
                    contact_info=translated_contact,
                )
            )
        return translated


# ---------------------------------------------------------------------------
# Module-level utility functions
# ---------------------------------------------------------------------------


def _build_language_hints(language: str) -> list[str]:
    """Build a list of language hints for the Vision OCR engine.

    Indian documents often contain text in multiple languages/scripts
    (e.g. a Hindi document with English headings and Urdu annotations).
    We always include English as a secondary hint.
    """
    # Map from ISO 639-1 to BCP-47 codes that Vision OCR understands.
    lang_hint_map: Final[dict[str, list[str]]] = {
        "hi": ["hi", "en"],          # Hindi + English
        "bn": ["bn", "en"],          # Bengali + English
        "te": ["te", "en"],          # Telugu + English
        "mr": ["mr", "hi", "en"],    # Marathi + Hindi + English
        "ta": ["ta", "en"],          # Tamil + English
        "ur": ["ur", "hi", "en"],    # Urdu + Hindi + English
        "gu": ["gu", "en"],          # Gujarati + English
        "kn": ["kn", "en"],          # Kannada + English
        "or": ["or", "en"],          # Odia + English
        "ml": ["ml", "en"],          # Malayalam + English
        "pa": ["pa", "en"],          # Punjabi + English
        "as": ["as", "bn", "en"],    # Assamese + Bengali + English
        "ne": ["ne", "hi", "en"],    # Nepali + Hindi + English
        "sd": ["sd", "ur", "en"],    # Sindhi + Urdu + English
        "sa": ["sa", "hi", "en"],    # Sanskrit + Hindi + English
        "ks": ["ks", "ur", "en"],    # Kashmiri + Urdu + English
        "en": ["en"],                # English only
    }

    hints = lang_hint_map.get(language, [language, "en"])
    # Deduplicate while preserving order.
    seen: set[str] = set()
    deduplicated: list[str] = []
    for hint in hints:
        if hint not in seen:
            seen.add(hint)
            deduplicated.append(hint)
    return deduplicated


def _classify_document_type(text: str) -> DocumentType:
    """Preliminary document type classification using keyword matching.

    This is a fast, rule-based classifier that runs before the LLM
    analysis.  The LLM may override this classification with a more
    accurate result.  Keywords are matched case-insensitively and
    include both English and Hindi/Devanagari terms.
    """
    text_lower = text.lower()

    # Order matters: more specific patterns first.
    keyword_map: list[tuple[list[str], DocumentType]] = [
        # Warrants and arrest-related
        (
            ["warrant", "वारंट", "गिरफ्तारी वारंट", "arrest warrant"],
            DocumentType.WARRANT,
        ),
        # Summons
        (
            ["summon", "समन", "सम्मन", "huzuri"],
            DocumentType.SUMMONS,
        ),
        # Bail orders
        (
            ["bail order", "bail bond", "जमानत", "ज़मानत आदेश"],
            DocumentType.BAIL_ORDER,
        ),
        # FIR / police complaint
        (
            ["first information report", "f.i.r", "fir no", "एफ.आई.आर",
             "प्रथम सूचना रिपोर्ट", "fir ", "zero fir"],
            DocumentType.FIR,
        ),
        # Police complaint (non-FIR)
        (
            ["police complaint", "complaint no", "शिकायत"],
            DocumentType.POLICE_COMPLAINT,
        ),
        # Eviction notice
        (
            ["eviction", "बेदखली", "notice to vacate", "立退き",
             "evict", "बेदखल"],
            DocumentType.EVICTION_NOTICE,
        ),
        # Court notice
        (
            ["court notice", "न्यायालय नोटिस", "judicial notice",
             "अदालत", "न्यायालय", "court of"],
            DocumentType.COURT_NOTICE,
        ),
        # Court order / judgment
        (
            ["court order", "judgment", "decree", "आदेश",
             "न्यायिक आदेश", "निर्णय"],
            DocumentType.COURT_ORDER,
        ),
        # Show cause notice
        (
            ["show cause", "कारण बताओ", "show-cause"],
            DocumentType.SHOW_CAUSE_NOTICE,
        ),
        # Demand notice
        (
            ["demand notice", "मांग नोटिस", "demand for payment"],
            DocumentType.DEMAND_NOTICE,
        ),
        # Tax notice
        (
            ["income tax", "gst notice", "tax notice", "आयकर",
             "कर नोटिस", "assessment order", "section 143",
             "section 148"],
            DocumentType.TAX_NOTICE,
        ),
        # Land record / revenue record
        (
            ["khata", "khasra", "खतौनी", "खसरा", "7/12", "jamabandi",
             "जमाबंदी", "fard", "mutation", "land record",
             "भू-अभिलेख", "bhulekh", "patta", "पट्टा"],
            DocumentType.LAND_RECORD,
        ),
        # Mutation order
        (
            ["mutation order", "दाखिल खारिज", "नामांतरण"],
            DocumentType.MUTATION_ORDER,
        ),
        # Property deed
        (
            ["sale deed", "registry", "बैनामा", "विक्रय पत्र",
             "conveyance deed", "gift deed", "lease deed"],
            DocumentType.PROPERTY_DEED,
        ),
        # Pension order
        (
            ["pension", "पेंशन", "pensioner"],
            DocumentType.PENSION_ORDER,
        ),
        # Ration card
        (
            ["ration card", "राशन कार्ड", "nfsa card", "bpl card",
             "apl card"],
            DocumentType.RATION_CARD,
        ),
        # Caste certificate
        (
            ["caste certificate", "जाति प्रमाण", "sc/st certificate",
             "obc certificate"],
            DocumentType.CASTE_CERTIFICATE,
        ),
        # Income certificate
        (
            ["income certificate", "आय प्रमाण पत्र"],
            DocumentType.INCOME_CERTIFICATE,
        ),
        # Domicile certificate
        (
            ["domicile", "निवास प्रमाण", "residence certificate"],
            DocumentType.DOMICILE_CERTIFICATE,
        ),
        # Birth certificate
        (
            ["birth certificate", "जन्म प्रमाण"],
            DocumentType.BIRTH_CERTIFICATE,
        ),
        # Death certificate
        (
            ["death certificate", "मृत्यु प्रमाण"],
            DocumentType.DEATH_CERTIFICATE,
        ),
        # Challan
        (
            ["challan", "चालान", "traffic challan", "e-challan"],
            DocumentType.CHALLAN,
        ),
        # Bank notice
        (
            ["bank notice", "loan recall", "npa notice",
             "बैंक नोटिस", "sarfaesi"],
            DocumentType.BANK_NOTICE,
        ),
        # Insurance document
        (
            ["insurance", "बीमा", "policy document", "claim"],
            DocumentType.INSURANCE_DOCUMENT,
        ),
        # Scheme sanction / approval letter
        (
            ["sanction letter", "scheme approved", "स्वीकृति पत्र",
             "benefit sanctioned", "yojana approved"],
            DocumentType.SCHEME_SANCTION_LETTER,
        ),
        # Scheme rejection letter
        (
            ["application rejected", "scheme rejected", "अस्वीकृत",
             "not eligible", "rejection letter"],
            DocumentType.SCHEME_REJECTION_LETTER,
        ),
        # Employment letter
        (
            ["appointment letter", "offer letter", "employment",
             "नियुक्ति पत्र"],
            DocumentType.EMPLOYMENT_LETTER,
        ),
        # Labour notice
        (
            ["labour notice", "श्रम नोटिस", "industrial dispute",
             "esi notice", "pf notice", "epfo"],
            DocumentType.LABOUR_NOTICE,
        ),
        # Government order / letter (generic -- keep near end)
        (
            ["government order", "शासनादेश", "office order",
             "सरकारी आदेश", "g.o. no", "order no"],
            DocumentType.GOVERNMENT_ORDER,
        ),
        (
            ["government letter", "सरकारी पत्र", "office of the",
             "letter no", "d.o. letter"],
            DocumentType.GOVERNMENT_LETTER,
        ),
    ]

    for keywords, doc_type in keyword_map:
        for keyword in keywords:
            if keyword in text_lower:
                return doc_type

    return DocumentType.UNKNOWN


def _parse_analysis_response(raw: str) -> dict:
    """Parse structured JSON from the LLM's document analysis response.

    The LLM is prompted to return JSON, but it may wrap it in markdown
    code fences or include preamble text.  This function extracts the
    JSON object robustly.
    """
    # Strip markdown code fences if present.
    cleaned = raw.strip()
    if cleaned.startswith("```"):
        # Remove opening fence (possibly with language hint like ```json).
        first_newline = cleaned.find("\n")
        if first_newline > 0:
            cleaned = cleaned[first_newline + 1:]
        # Remove closing fence.
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3].strip()

    # Attempt direct parse.
    try:
        return json.loads(cleaned)
    except (json.JSONDecodeError, ValueError):
        pass

    # Try to find a JSON object in the text.
    try:
        start_idx = cleaned.find("{")
        end_idx = cleaned.rfind("}") + 1
        if start_idx >= 0 and end_idx > start_idx:
            json_str = cleaned[start_idx:end_idx]
            return json.loads(json_str)
    except (json.JSONDecodeError, ValueError):
        pass

    logger.warning(
        "document_scanner.analysis_parse_failed",
        raw_length=len(raw),
        raw_preview=raw[:200],
    )

    # Fallback: return the raw text as the summary.
    return {
        "summary": raw,
        "action_items": [],
        "deadlines": [],
        "referenced_laws": [],
        "referenced_schemes": [],
        "plain_language_summary": raw,
    }


def _get_disclaimer(language: str) -> str:
    """Return the legal disclaimer in the appropriate language.

    Pre-translated disclaimers are available for English and Hindi.
    For other languages, the English disclaimer is returned -- the
    caller may translate it via the :class:`TranslationService`.
    """
    if language == "hi":
        return LEGAL_DISCLAIMER_HI
    return LEGAL_DISCLAIMER
